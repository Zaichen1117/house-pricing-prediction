{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[1Ô∏è‚É£ Data Preparation](#toc0_)\n",
    "<!-- **Designed by:** [datamover.ai](https://www.datamover.ai)  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make script reproducible\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Fetch dataset** \n",
    "\n",
    "Download dataset from this [url](https://www.kaggle.com/datasets/thomasnibb/amsterdam-house-price-prediction) and load the data in a `pd.DataFrame`. \n",
    "\n",
    "üíÅ‚Äç‚ôÇÔ∏è Checked this [article](https://www.datamover.ai/post/the-right-way-to-set-absolute-path-in-python) to learn how to load dataset OS agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>Room</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blasiusstraat 8 2, Amsterdam</td>\n",
       "      <td>1091 CR</td>\n",
       "      <td>685000.0</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>4.907736</td>\n",
       "      <td>52.356157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kromme Leimuidenstraat 13 H, Amsterdam</td>\n",
       "      <td>1059 EL</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>4.850476</td>\n",
       "      <td>52.348586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zaaiersweg 11 A, Amsterdam</td>\n",
       "      <td>1097 SM</td>\n",
       "      <td>850000.0</td>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>4.944774</td>\n",
       "      <td>52.343782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tenerifestraat 40, Amsterdam</td>\n",
       "      <td>1060 TH</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>4.789928</td>\n",
       "      <td>52.343712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Winterjanpad 21, Amsterdam</td>\n",
       "      <td>1036 KN</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>138</td>\n",
       "      <td>5</td>\n",
       "      <td>4.902503</td>\n",
       "      <td>52.410538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Address      Zip     Price  Area  Room  \\\n",
       "1            Blasiusstraat 8 2, Amsterdam  1091 CR  685000.0    64     3   \n",
       "2  Kromme Leimuidenstraat 13 H, Amsterdam  1059 EL  475000.0    60     3   \n",
       "3              Zaaiersweg 11 A, Amsterdam  1097 SM  850000.0   109     4   \n",
       "4            Tenerifestraat 40, Amsterdam  1060 TH  580000.0   128     6   \n",
       "5              Winterjanpad 21, Amsterdam  1036 KN  720000.0   138     5   \n",
       "\n",
       "        Lon        Lat  \n",
       "1  4.907736  52.356157  \n",
       "2  4.850476  52.348586  \n",
       "3  4.944774  52.343782  \n",
       "4  4.789928  52.343712  \n",
       "5  4.902503  52.410538  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "FILENAME = \"HousingPrices-Amsterdam-August-2021.csv\"\n",
    "\n",
    "data = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, FILENAME),\n",
    "    index_col=0,\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Check dataset size and ensure workspace has enough storage even when dealing with big datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size data: 0.18 Mb\n"
     ]
    }
   ],
   "source": [
    "size_b = data.memory_usage(deep=True).sum()  # get size in byte\n",
    "size_mb = size_b / (1024 * 1024)  # convert byte to mb\n",
    "print(f\"Size data: {size_mb:.2f} Mb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù `deep=True` introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Check type of data (time series, sample, geographical, etc.) and make sure they are what they should be.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 924 entries, 1 to 924\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Address  924 non-null    object \n",
      " 1   Zip      924 non-null    object \n",
      " 2   Price    920 non-null    float64\n",
      " 3   Area     924 non-null    int64  \n",
      " 4   Room     924 non-null    int64  \n",
      " 5   Lon      924 non-null    float64\n",
      " 6   Lat      924 non-null    float64\n",
      "dtypes: float64(3), int64(2), object(2)\n",
      "memory usage: 57.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. If necessary, convert the data to a format that is easy to manipulate (without changing the data itself, e.g. .csv, .json).**\n",
    "\n",
    "In this case the dataset is already in a format easy to manipulate, i.e., `.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. For training of ML models, sample a hold-out set, put it aside, and <font color='#E7240E'>never look at it</font> ‚ö†Ô∏è.**\n",
    "\n",
    "- typical train/test splits are `60/40`, `70/30`, `80/20`;\n",
    "- it is convenient to store train and test data separately;\n",
    "- üìù often test set and hold-out are terms used interchangeably.\n",
    "\n",
    "<ins> For this project aim to have a 60/40 train/test split ratio. <ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Price\"  # get target name\n",
    "\n",
    "# split data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=[TARGET]), data[TARGET], test_size=0.40, random_state=42\n",
    ")\n",
    "\n",
    "# re-merge X,y for both train and test\n",
    "data_train = pd.merge(left=y_train, right=X_train, left_index=True, right_index=True)\n",
    "data_test = pd.merge(left=y_test, right=X_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sample train set: 554 (0.60%) \n",
      "# sample test set: 370 (0.40%) \n"
     ]
    }
   ],
   "source": [
    "# Check sample size of each set with corresponding percentage\n",
    "print(\n",
    "    f\"# sample train set: {data_train.shape[0]} ({data_train.shape[0]/len(data):.2f}%) \"\n",
    ")\n",
    "print(f\"# sample test set: {data_test.shape[0]} ({data_test.shape[0]/len(data):.2f}%) \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. In a data-rich situation, one can create additional splits called *validation set* (usually equal in size to the hold-out set) to estimate the prediction error for model selection. If dealing with a classification problem, one should also put aside two additional sets: *calibration* and *calibration-validation*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split the test data in validtion (50%) and test (50%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    data_test.drop(columns=[TARGET]), data_test[TARGET], test_size=0.50, random_state=42\n",
    ")\n",
    "\n",
    "# re-merge X,y for both val and test\n",
    "data_val = pd.merge(left=y_val, right=X_val, left_index=True, right_index=True)\n",
    "data_test = pd.merge(left=y_test, right=X_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sample train set: 554 (0.60%) \n",
      "# sample val set: 185 (0.20%) \n",
      "# sample test set: 185 (0.20%) \n"
     ]
    }
   ],
   "source": [
    "# Double check sample size of each set with corresponding percentage\n",
    "print(\n",
    "    f\"# sample train set: {data_train.shape[0]} ({data_train.shape[0]/len(data):.2f}%) \"\n",
    ")\n",
    "print(f\"# sample val set: {data_val.shape[0]} ({data_val.shape[0]/len(data):.2f}%) \")\n",
    "print(f\"# sample test set: {data_test.shape[0]} ({data_test.shape[0]/len(data):.2f}%) \")\n",
    "\n",
    "# assert the the size of each set is correct\n",
    "assert len(data) == len(data_train) + len(data_val) + len(data_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. ‚¨áÔ∏è Store train and test locally**\n",
    "- Store both datasets in `data` folder in `csv` format;\n",
    "- Save train and test set as `data_train.csv` and `data_set.csv`, respectively (and any additional set created in the previous step).\n",
    "- For each dataset, retain the column names and discard the index if it is not informative.\n",
    "\n",
    "üíÅ‚Äç‚ôÇÔ∏è Automate scripts as much as possible for future data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data folder if not exists\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# save data\n",
    "data_train.to_csv(\n",
    "    path_or_buf=\"./data/data_train.csv\",\n",
    "    header=True,  # Write out the column names\n",
    "    index=False,  # discard index as it is not informative\n",
    ")\n",
    "\n",
    "data_test.to_csv(\n",
    "    path_or_buf=\"./data/data_test.csv\",\n",
    "    header=True,  # Write out the column names\n",
    "    index=False,  # discard index as it is not informative\n",
    ")\n",
    "\n",
    "data_val.to_csv(\n",
    "    path_or_buf=\"./data/data_val.csv\",\n",
    "    header=True,  # Write out the column names\n",
    "    index=False,  # discard index as it is not informative\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "play-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
